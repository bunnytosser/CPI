import pandas as pd
# original=pd.read_csv("DTC_data_filter.csv",sep="\t")
import pandas as pd
from scipy import stats
from scipy.stats import norm
import pandas as pd
import numpy as np
import deepchem
import copy
import sklearn as sk
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import recall_score
from sklearn.pipeline import make_pipeline
from scipy.stats import pearsonr
def average_AUC(y,f):
    thr = np.linspace(6,8,10)
    auc = np.empty(np.shape(thr)); auc[:] = np.nan
    for i in range(len(thr)):
        y_binary = copy.deepcopy(y)
        y_binary = preprocessing.binarize(y_binary.reshape(1,-1), threshold=thr[i], copy=False)[0]
        fpr, tpr, thresholds = metrics.roc_curve(y_binary, f, pos_label=1)
        auc[i] = metrics.auc(fpr, tpr)
#     print(auc)
    avAUC = np.nanmean(auc)
    return avAUC

def f1_s(y,f):
    y_binary = copy.deepcopy(y)
    y_binary = preprocessing.binarize(y_binary.reshape(1,-1), threshold=7.0, copy=False)[0]
    f_binary = copy.deepcopy(f)
    f_binary = preprocessing.binarize(f_binary.reshape(1,-1), threshold=7.0, copy=False)[0]
    f1 = metrics.f1_score(y_binary, f_binary)
    return f1

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA

from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn import svm
from collections import defaultdict
from sklearn.metrics import make_scorer
from sklearn.preprocessing import MaxAbsScaler,StandardScaler,RobustScaler
import pickle
from sklearn.externals import joblib
from scipy.sparse import csr_matrix
from sklearn.ensemble import GradientBoostingClassifier
from ast import literal_eval
import warnings
from sklearn.calibration import CalibratedClassifierCV
from sklearn.svm import LinearSVC

from sklearn.metrics import f1_score
import numpy as np
import copy
from math import sqrt
from scipy import stats
from sklearn import preprocessing,metrics
import seaborn as sns
aucs=[]
spearmans=[]
rmses=[]
pearsons=[]
f1s=[]
aucs1=[]
spearmans1=[]
rmses1=[]
pearsons1=[]
f1s1=[]
def svmtxt_converter(filename):
    with open(filename+".txt") as fin, open(filename+".svm.txt", 'w') as fout:
        for line in fin:
            fout.write(line.replace(',', ' ').replace("\t"," "))
def average_AUC(y,f):
    """
    Task:    To compute average area under the ROC curves (AUC) given ten
             interaction threshold values from the pKd interval [6 M, 8 M]
             to binarize pKd's into true class labels.
    Input:   y      Vector with original labels (pKd [M])
             f      Vector with predicted labels (pKd [M])
    Output:  avAUC   average AUC
    """
    thr = np.linspace(6,8,10)
    auc = np.empty(np.shape(thr)); auc[:] = np.nan
    for i in range(len(thr)):
        y_binary = copy.deepcopy(y)
        y_binary = preprocessing.binarize(y_binary.reshape(1,-1), threshold=thr[i], copy=False)[0]
        fpr, tpr, thresholds = metrics.roc_curve(y_binary, f, pos_label=1)
        auc[i] = metrics.auc(fpr, tpr)
#     print(auc)
    avAUC = np.nanmean(auc)
    return avAUC

def f1(y,f):
    y_binary = copy.deepcopy(y)
    y_binary = preprocessing.binarize(y_binary.reshape(1,-1), threshold=7.0, copy=False)[0]
    f_binary = copy.deepcopy(f)
    f_binary = preprocessing.binarize(f_binary.reshape(1,-1), threshold=7.0, copy=False)[0]
    f1 = metrics.f1_score(y_binary, f_binary)
    return f1


def eval(ypred,yreal):
    auc=[]
    for thr in np.arange(6,8,0.2):
        truelabel=np.where(yreal>thr,1,0)
        predlabel=np.where(ypred>thr,1,0)
        auc.append(metrics.roc_auc_score(truelabel,predlabel))
    from sklearn.metrics import r2_score
#     print("r2 score",r2_score(yreal,ypred))
#     print("mean auc",np.mean(auc))
    print("f1",f1(yreal,ypred))
    f1s.append(f1(yreal,ypred))
    from scipy.stats.stats import pearsonr,spearmanr
    from sklearn.metrics import mean_squared_error
    from math import sqrt
    print("auc",average_AUC(yreal,ypred))
    aucs.append(average_AUC(yreal,ypred))
    print("pearson",pearsonr(ypred,yreal)[0])
    pearsons.append(pearsonr(ypred,yreal)[0])
    print("spearman",spearmanr(ypred,yreal)[0])
    spearmans.append(spearmanr(ypred,yreal)[0])
    print("rmse",sqrt(mean_squared_error(ypred, yreal)))
    rmses.append(sqrt(mean_squared_error(ypred, yreal)))
    
def eval1(ypred,yreal):
    auc=[]
    for thr in np.arange(6,8,0.2):
        truelabel=np.where(yreal>thr,1,0)
        predlabel=np.where(ypred>thr,1,0)
        auc.append(metrics.roc_auc_score(truelabel,predlabel))
    from sklearn.metrics import r2_score
#     print("r2 score",r2_score(yreal,ypred))
#     print("mean auc",np.mean(auc))
    print("f1",f1(yreal,ypred))
    f1s1.append(f1(yreal,ypred))
    from scipy.stats.stats import pearsonr,spearmanr
    from sklearn.metrics import mean_squared_error
    from math import sqrt
    print("auc",average_AUC(yreal,ypred))
    aucs1.append(average_AUC(yreal,ypred))
    print("pearson",pearsonr(ypred,yreal)[0])
    pearsons1.append(pearsonr(ypred,yreal)[0])
    print("spearman",spearmanr(ypred,yreal)[0])
    spearmans1.append(spearmanr(ypred,yreal)[0])
    print("rmse",sqrt(mean_squared_error(ypred, yreal)))
    rmses1.append(sqrt(mean_squared_error(ypred, yreal)))

def get_importance(bst):
    importances=bst.get_score(importance_type='gain')
    importances=pd.DataFrame.from_dict(importances,orient="index")
    print(importances.sort_values(0,ascending=False))
    return(importances.sort_values(0,ascending=False))
def round_plot(bst):
    dpred = xgb.DMatrix('predict.svm.txt')
    predicted_kd=bst.predict(dpred)
#     import seaborn as sns
    sns.distplot(predicted_kd,bins=17)
    
def xgboost_trainer(dtrain,dtest,num_round=200,lam=100,max_depth=3,gamma=1,nthread=4,subsample=0.8,alpha=1):
    aucs=[]
    spearmans=[]
    rmses=[]
    pearsons=[]
    f1s=[]
    aucs1=[]
    spearmans1=[]
    rmses1=[]
    pearsons1=[]
    f1s1=[]
    num_round = num_round
    param = {"verbosity":2,"feature_selector":"shuffle", 'nthread':nthread,'silent': 1, 'objective': 'reg:linear',
                    "early_stopping_rounds": 2,"eval_metric":"rmse","seed":92,"subsample":subsample,'max_depth': max_depth,
                   'eta': 0.1,"gamma":gamma,"n_estimators":100,"alpha":alpha,"lambda":lam,"colsample_bytree":0.2,
             "min_child_weight":2}
    evallist = [(dtest, 'eval'), (dtrain, 'train')]
    bst = xgb.train(param, dtrain, num_round,evallist)
    pkl_filename ="pkl0.pkl"
    with open(pkl_filename, 'wb') as file:
        pickle.dump(bst, file)
    model=pickle.load(open(pkl_filename,"rb"))
    ypred = model.predict(dtrain)
    # sns.distplot(ypred)
    yreal=dtrain.get_label()
    print("training...")
    eval1(ypred,yreal)
    print("testing...")
    ypred = model.predict(dtest)
#     sns.distplot(ypred)
    yreal=dtest.get_label()
    eval(ypred,yreal)
    get_importance(bst)
    round_plot(bst)
    return(bst)
import math
from sklearn.metrics import f1_score
from scipy.stats import pearsonr,spearmanr
from sklearn.metrics import mean_squared_error
from math import sqrt
def f1_s(y,f):
    y_binary = copy.deepcopy(y)
    y_binary = preprocessing.binarize(y_binary.reshape(1,-1), threshold=7.0, copy=False)[0]
    f_binary = copy.deepcopy(f)
    f_binary = preprocessing.binarize(f_binary.reshape(1,-1), threshold=7.0, copy=False)[0]
    f1 = metrics.f1_score(y_binary, f_binary)
    return f1
def post_estimation(model,test_dataset):
    f=model.predict(test_dataset)
    f0=f[:,0]
    y0=test_dataset.y[:,0]
    y1=test_dataset.y[:,1]
    f1=f[:,1]
    f2=f[:,2]
    y2=test_dataset.y[:,2]
    ind_0=test_dataset.y[:,0].nonzero()
    ind_1=test_dataset.y[:,1].nonzero()
    ind_2=test_dataset.y[:,2].nonzero()
    y_0=y0[ind_0]
    y_1=y1[ind_1]
    y_2=y2[ind_2]
    f_0=f0[ind_0]
    f_1=f1[ind_1]
    f_2=f2[ind_2]
    f_0=f_0.reshape(len(f_0),)
    f_1=f_1.reshape(len(f_1),)
    f_2=f_2.reshape(len(f_2),)
    aucs=[average_AUC(y_0,f_0),average_AUC(y_1,f_1),average_AUC(y_2,f_2)]
    f1s=[f1_s(y_0,f_0),f1_s(y_1,f_1),f1_s(y_2,f_2)]
    rmses=[sqrt(mean_squared_error(y_0,f_0)),sqrt(mean_squared_error(y_1,f_1)),sqrt(mean_squared_error(y_2,f_2))]
    pearsons=[pearsonr(y_0,f_0)[0],pearsonr(y_1,f_1)[0],pearsonr(y_2,f_2)[0]]
    spearmans=[spearmanr(y_0,f_0)[0],spearmanr(y_1,f_1)[0],spearmanr(y_2,f_2)[0]]
    evaluation=pd.DataFrame({"aucs":aucs,"f1s":f1s,"rmses":rmses,"pearsons":pearsons,"spearmans":spearmans})
    print("AUC",average_AUC(y_0,f_0),average_AUC(y_1,f_1),average_AUC(y_2,f_2))
    print("f1 score",f1_s(y_0,f_0),f1_s(y_1,f_1),f1_s(y_2,f_2))
    print("rmse",sqrt(mean_squared_error(y_0,f_0)),sqrt(mean_squared_error(y_1,f_1)),sqrt(mean_squared_error(y_2,f_2)))
    print("pearsonr",pearsonr(y_0,f_0)[0],pearsonr(y_1,f_1)[0],pearsonr(y_2,f_2)[0])
    print("spearmanr",spearmanr(y_0,f_0)[0],spearmanr(y_1,f_1)[0],spearmanr(y_2,f_2)[0])
    evaluation.index=["ic50","ki","kd"]
    return evaluation

def post_estimation_kd(model,test_dataset):
    f=model.predict(test_dataset)
    f2=f[:,2]
    y2=test_dataset.y[:,2]
    print("AUC",average_AUC(y2,f2))
    print("f1 score",f1_s(y2,f2))
    print("rmse",sqrt(mean_squared_error(y2,f2)))
    print("pearsonr",pearsonr(y2,f2)[0])
    print("spearmanr",spearmanr(y2,f2)[0])
def post_estimation_kd2(model,test_dataset):
    f=model.predict(test_dataset)
    f2=f[:,2].reshape(len(f),)
    y2=test_dataset.y[:,2].reshape(len(f),)
    print("AUC",average_AUC(y2,f2))
    print("f1 score",f1_s(y2,f2))
    print("rmse",sqrt(mean_squared_error(y2,f2)))
    print("pearsonr",pearsonr(y2,f2)[0])
    print("spearmanr",spearmanr(y2,f2)[0])
    
def post_estimation_ic50(model,test_dataset):
    f=model.predict(test_dataset)
    f0=f[:,0]
    y0=test_dataset.y[:,2]
    aucs=average_AUC(y0,f0)
    f1s=f1_s(y0,f0)
    rmses=sqrt(mean_squared_error(y0,f0))
    pearsons=pearsonr(y0,f0)[0]
    spearmans=spearmanr(y0,f0)[0]
    print("AUC",aucs)
    print("f1 score",f1s)
    print("rmse",rmses)
    print("pearsonr",pearsons)
    print("spearmanr",spearmans)

#load data
import tensorflow as tf
import deepchem as dc
import numpy as np
final_features= list(pd.read_csv("feas_name_map.txt",header=None,sep="\t").iloc[:,1])[:5236]
final_features[5127]='Unnamed: 5128'
graph_featurizer = dc.feat.graph_features.ConvMolFeaturizer()
molfea=dc.feat.basic.MolecularWeight()
featurizer = dc.feat.UserDefinedFeaturizer(final_features)
# loader = dc.data.data_loader.UserCSVLoader( tasks=["ic50","ki","kd"], id_field="ind",featurizer=featurizer)
loader = dc.data.data_loader.UserCSVLoader( tasks=["ic50","ki","kd"], id_field="ind",featurizer=featurizer)

dataset_0 = loader.featurize(['family/7_0325_KI_pk.csv0.0.csv.svm.txt.csv',
                            'family/7_0325_KD_pk.csv0.0.csv.svm.txt.csv',
                              'family/7_0325_IC50_pk.csv0.0.csv.svm.txt.csv'],shard_size=4000)
dataset_1 = loader.featurize(['family/7_0325_KI_pk.csv1.0.csv.svm.txt.csv',
                            'family/7_0325_KD_pk.csv1.0.csv.svm.txt.csv',
                              'family/7_0325_IC50_pk.csv1.0.csv.svm.txt.csv'],shard_size=4000)
dataset_2 = loader.featurize(['family/7_0325_KI_pk.csv2.0.csv.svm.txt.csv',
                            'family/7_0325_KD_pk.csv2.0.csv.svm.txt.csv',
                              'family/7_0325_IC50_pk.csv2.0.csv.svm.txt.csv'],shard_size=4000)
dataset_3 = loader.featurize(['family/7_0325_KI_pk.csv3.0.csv.svm.txt.csv',
                            'family/7_0325_KD_pk.csv3.0.csv.svm.txt.csv',
                              'family/7_0325_IC50_pk.csv3.0.csv.svm.txt.csv'],shard_size=4000)
dataset_4 = loader.featurize(['family/7_0325_KI_pk.csv4.0.csv.svm.txt.csv',
                            'family/7_0325_KD_pk.csv4.0.csv.svm.txt.csv',
                              'family/7_0325_IC50_pk.csv4.0.csv.svm.txt.csv'],shard_size=4000)
from deepchem.data import DiskDataset
from deepchem.data import DiskDataset
train_total=DiskDataset.merge([dataset_3,dataset_0,dataset_4,dataset_2,dataset_1])
from __future__ import division
from __future__ import unicode_literals
splitter = dc.splits.splitters.RandomSplitter()
train_dataset,test_dataset = splitter.train_test_split(train_total)
import numpy as np
import deepchem

import os
import logging
import time
from deepchem.molnet.load_function.kaggle_features import merck_descriptors

logger = logging.getLogger(__name__)


def remove_missing_entries(dataset):
  """Remove missing entries.

  Some of the datasets have missing entries that sneak in as zero'd out
  feature vectors. Get rid of them.
  """
  for i, (X, y, w, ids) in enumerate(dataset.itershards()):
    available_rows = X.any(axis=1)
    logger.info("Shard %d has %d missing entries." %
                (i, np.count_nonzero(~available_rows)))
    X = X[available_rows]
    y = y[available_rows]
    w = w[available_rows]
    ids = ids[available_rows]
    dataset.set_shard(i, X, y, w, ids)


def get_transformers(train_dataset):
  """Get transformers applied to datasets."""
  transformers = []
  #transformers = [
  #    deepchem.trans.LogTransformer(transform_X=True),
  #    deepchem.trans.NormalizationTransformer(transform_y=True,
  #                                      dataset=train_dataset)]
  return transformers
remove_missing_entries(train_dataset)
logger.info("Shuffling order of train dataset.")
train_dataset.sparse_shuffle()
logger.info("Transforming datasets with transformers.")
transformers = get_transformers(train_dataset)#####??????
logger.info("Transforming datasets with transformers.")
transformers = get_transformers(train_dataset)
for transformer in transformers:
    logger.info("Performing transformations with %s" % transformer.__class__.__name__)
    logger.info("Transforming datasets")
    train_dataset = transformer.transform(train_dataset)
    test_dataset = transformer.transform(test_dataset)
test_dataset.get_shape()

#progressive network
num_trials = 1
metric = dc.metrics.Metric(dc.metrics.pearson_r2_score, task_averager=np.mean)
all_results = []
for trial in range(num_trials):
    n_layers = 3
    nb_epoch = 30
    modelm1 = dc.models.ProgressiveMultitaskRegressor(
      3,
      train_total.get_data_shape()[0],
      layer_sizes=[200] * n_layers,
      dropouts=[.25] * n_layers,
      alpha_init_stddevs=[.02] * n_layers,
      weight_init_stddevs=[.02] * n_layers,
      bias_init_consts=[1.] * n_layers,
      learning_rate=.0005,
      batch_size=100,
    model_dir="100*2p")
    print("Fitting Model")
    modelm1.fit(train_total, nb_epoch=nb_epoch)
    modelm1.save()
######Robust Model
n_layers = 3
n_bypass_layers = 2
nb_epoch = 10
num_trials=1
bypass_size=80
learning_r=0.0005
import copy
#Use R2 classification metric
from scipy.stats import pearsonr
metric = dc.metrics.Metric(dc.metrics.pearson_r2_score, task_averager=np.mean)
all_results = []
for trial in range(num_trials):
    model = dc.models.RobustMultitaskRegressor(
          3,
          train_total.get_data_shape()[0],
          layer_sizes=[80,80],
          bypass_layer_sizes=[bypass_size] * n_bypass_layers,
          dropouts=[.25] * n_layers,
          bypass_dropouts=[.25] * n_bypass_layers,
          weight_init_stddevs=[.02] * n_layers,
          bias_init_consts=[1.] * n_layers,
          bypass_weight_init_stddevs=[.02] * n_bypass_layers,
          bypass_bias_init_consts=[1.] * n_bypass_layers,
          learning_rate=learning_r,
          weight_decay_penalty=.0005,
          weight_decay_penalty_type="l2",
          batch_size=100,
    model_dir="80*80")
    print("Fitting bypass Model")
    model.fit(train_total, nb_epoch=nb_epoch)
    model.save()
  
#visualization
#
model=model.load_from_dir('200_100_100_150_150')
print("robust")
post_estimation_kd(model,test2)
post_estimation_ic50(model,testnew)

import matplotlib.pyplot as plt
import seaborn as sns
predicted=model.predict(round2)
plt.figure(figsize=(6, 4))
sns.distplot(pd.Series(predicted[:,2]),color="lightseagreen",bins=20)
plt.title("round 2 (robust)")
print("round 2 average:",predicted[:,2].mean())

import matplotlib.pyplot as plt
import seaborn as sns
predicted=model.predict(round2)
predicted2=modelm.predict(round2)
rp=predicted[:,2]
pp=predicted2[:,2].reshape(len(rp),)
ap=(rp+pp)/2
plt.figure(figsize=(10, 6))
sns.distplot(pd.Series(predicted[:,2]),color="lightseagreen",label="robust")
# import matplotlib.pyplot as plt
# import seaborn as sns

sns.distplot(pd.Series(predicted2[:,2].reshape(len(round2),)),color="orange",label="progressive")
plt.legend()
print(predicted[:,2].mean())

print(predicted2[:,2].mean())
jp = sns.jointplot(x=pd.Series(predicted[:,2]),  
                   y=pd.Series(predicted2[:,2].reshape(len(predicted),)), 
                   kind='kde', # <== 😀 KDE
#                    annot_kws=dict(stat="r"),
#                    marginal_kws=dict(bins=20),
                   space=0, size=6, ratio=4,color="lightcyan").set_axis_labels("robust","progressive")
jp.plot_joint(sns.regplot,color="darkgrey",marker="+")

app=pd.read_csv("09042019.csv")["pKd_[M]_pred"]
jp = sns.jointplot(x=pd.Series(ap), 
                   y=pd.Series(app), 
                   kind='kde', # <== 😀 KDE
#                    annot_kws=dict(stat="r"),
#                    marginal_kws=dict(bins=20),
                   space=0, size=6, ratio=4,color="steelblue").set_axis_labels("new features","old features")
jp.plot_joint(sns.regplot,color="white",marker="+")

modelm=dc.models.ProgressiveMultitaskRegressor.load_from_dir('120*2p')
predicted=modelm.predict(round2)
r2=pd.read_csv("round_2_template_feas_20190402.txt",sep="\t")
r2=r2.iloc[:,:6]
r2["pKd_[M]_pred"]=predicted[:,2].reshape(len(predicted),)
plt.figure(figsize=(25, 10))
sns.set(rc={'figure.figsize':(11.7,18.7)})
sns.boxplot(y=r2.Compound_SMILES,x=r2["pKd_[M]_pred"],palette="pastel")
r2.to_csv("15042019_m.csv")

import matplotlib.pyplot as plt
import seaborn as sns
predicted=model.predict(testnew)
# fig, ax =plt.subplots(1,2)
plt.figure(figsize=(6, 3))
g=sns.distplot(pd.Series(predicted[:,0]),color="salmon",bins=17,label="predicted").set_title("distribution of predicted/real target")
g=sns.distplot(pd.Series(testnew.y[:,2]),color="cyan",bins=17,label="real")
plt.legend()

print(predicted[:,0].mean())

jp = sns.jointplot(x=pd.Series(predicted[:,0]), 
                   y=pd.Series(testnew.y[:,2]), 
                   kind='kde', # <== 😀 KDE
#                    annot_kws=dict(stat="r"),
#                    marginal_kws=dict(bins=20),
                   space=0, size=6, ratio=4,color="lightseagreen").set_axis_labels("Predicted","Real")
jp.plot_joint(plt.scatter,c="w",marker="+")
